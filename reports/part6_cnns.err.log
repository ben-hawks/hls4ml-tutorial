Traceback (most recent call last):
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/nbclient/client.py", line 1305, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/jupyter_core/utils/__init__.py", line 166, in wrapped
    return loop.run_until_complete(inner)
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/nbclient/client.py", line 705, in async_execute
    await self.async_execute_cell(
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/nbclient/client.py", line 1058, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/nbclient/client.py", line 914, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
from qkeras.autoqkeras import AutoQKeras

autoqk = AutoQKeras(baseline_model, output_dir="autoq_cnn", metrics=["acc"], custom_objects={}, **run_config)
autoqk.fit(train_data, validation_data=val_data, epochs=15)

aqmodel = autoqk.get_best_model()
print_qmodel_summary(aqmodel)

# Train for the full epochs
callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),
]

start = time.time()
history = aqmodel.fit(train_data, epochs=n_epochs, validation_data=val_data, callbacks=callbacks, verbose=1)
end = time.time()
print('\n It took {} minutes to train!\n'.format((end - start) / 60.0))
------------------

----- stdout -----
Trial 3 Complete [00h 00m 02s]

Best val_score So Far: 0.738520085811615
Total elapsed time: 00h 13m 02s

Search: Running Trial #4

Value             |Best Value So Far |Hyperparameter
quantized_bits(...|quantized_bits(...|conv_0_kernel_quantizer
quantized_bits(...|quantized_bits(...|conv_1_kernel_quantizer
quantized_bits(...|quantized_bits(...|conv_2_kernel_quantizer
quantized_bits(...|quantized_bits(...|dense_0_kernel_quantizer
quantized_bits(...|quantized_bits(...|dense_1_kernel_quantizer
quantized_bits(...|quantized_bits(...|output_dense_kernel_quantizer
1                 |1                 |network_filters_conv_0
quantized_relu(...|quantized_relu(...|conv_act_0_activation_quantizer
1                 |1                 |network_filters_conv_1
quantized_relu(...|quantized_relu(...|conv_act_1_activation_quantizer
2                 |2                 |network_filters_conv_2
quantized_relu(...|quantized_relu(...|conv_act_2_activation_quantizer
2                 |2                 |network_filters_dense_0
quantized_relu(...|quantized_relu(...|dense_act_0_activation_quantizer
0.75              |2                 |network_filters_dense_1
quantized_relu(...|quantized_relu(...|dense_act_1_activation_quantizer
quantized_bits(...|quantized_bits(...|output_dense_bias_quantizer
----- stdout -----
learning_rate: 0.003000000026077032
Model: "keras_baseline"
----- stdout -----
_________________________________________________________________
----- stdout -----
 Layer (type)                Output Shape              Param #
----- stdout -----
=================================================================
----- stdout -----
 input_3 (InputLayer)        [(None, 32, 32, 3)]       0
----- stdout -----

----- stdout -----
 conv_0 (QConv2D)            (None, 30, 30, 16)        432
----- stdout -----

----- stdout -----
 bn_conv_0 (BatchNormalizati  (None, 30, 30, 16)       64
----- stdout -----
 on)
----- stdout -----

----- stdout -----
 conv_act_0 (QActivation)    (None, 30, 30, 16)        0
----- stdout -----

----- stdout -----
 pool_0 (MaxPooling2D)       (None, 15, 15, 16)        0
----- stdout -----

----- stdout -----
 conv_1 (QConv2D)            (None, 13, 13, 16)        2304
----- stdout -----

----- stdout -----
 bn_conv_1 (BatchNormalizati  (None, 13, 13, 16)       64
----- stdout -----
 on)
----- stdout -----

----- stdout -----
 conv_act_1 (QActivation)    (None, 13, 13, 16)        0
----- stdout -----

----- stdout -----
 pool_1 (MaxPooling2D)       (None, 6, 6, 16)          0
----- stdout -----

----- stdout -----
 conv_2 (QConv2D)            (None, 4, 4, 48)          6912
----- stdout -----

----- stdout -----
 bn_conv_2 (BatchNormalizati  (None, 4, 4, 48)         192
----- stdout -----
 on)
----- stdout -----

----- stdout -----
 conv_act_2 (QActivation)    (None, 4, 4, 48)          0
----- stdout -----

----- stdout -----
 pool_2 (MaxPooling2D)       (None, 2, 2, 48)          0
----- stdout -----

----- stdout -----
 flatten_2 (Flatten)         (None, 192)               0
----- stdout -----

----- stdout -----
 dense_0 (QDense)            (None, 84)                16128
----- stdout -----

----- stdout -----
 bn_dense_0 (BatchNormalizat  (None, 84)               336
----- stdout -----
 ion)
----- stdout -----

----- stdout -----
 dense_act_0 (QActivation)   (None, 84)                0
----- stdout -----

----- stdout -----
 dense_1 (QDense)            (None, 48)                4032
----- stdout -----

----- stdout -----
 bn_dense_1 (BatchNormalizat  (None, 48)               192
----- stdout -----
 ion)
----- stdout -----

----- stdout -----
 dense_act_1 (QActivation)   (None, 48)                0
----- stdout -----

----- stdout -----
 output_dense (QDense)       (None, 10)                490
----- stdout -----

----- stdout -----
 output_softmax (Activation)  (None, 10)               0
----- stdout -----

----- stdout -----
=================================================================
----- stdout -----
Total params: 31,146
----- stdout -----
Trainable params: 30,722
----- stdout -----
Non-trainable params: 424
----- stdout -----
_________________________________________________________________
----- stdout -----
stats: delta_p=0.08 delta_n=0.08 rate=2.0 trial_size=1852936 reference_size=520009
       delta=-14.67%
Total Cost Reduction:
       1852936 vs 520009 (256.33%)
conv_0               f=16 quantized_bits(8,0,1,alpha=1.0) 
bn_conv_0            is normal keras bn layer
conv_act_0           quantized_relu(3,1)
conv_1               f=16 quantized_bits(6,0,1,alpha=1.0) 
bn_conv_1            is normal keras bn layer
conv_act_1           quantized_relu(3,1)
conv_2               f=48 quantized_bits(8,0,1,alpha=1.0) 
bn_conv_2            is normal keras bn layer
conv_act_2           quantized_relu(16,6)
dense_0              u=84 quantized_bits(4,0,1,alpha=1.0) 
bn_dense_0           is normal keras bn layer
dense_act_0          quantized_relu(8,4)
dense_1              u=48 quantized_bits(2,0,1,alpha=1.0) 
bn_dense_1           is normal keras bn layer
dense_act_1          quantized_relu(3,1)
output_dense         u=10 quantized_bits(4,0,1,alpha=1.0) quantized_bits(4,0,1,alpha=1.0) 

Epoch 1/15
----- stderr -----
Traceback (most recent call last):
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py", line 270, in _try_run_and_update_trial
    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py", line 235, in _run_and_update_trial
    results = self.run_trial(trial, *fit_args, **fit_kwargs)
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/tuner.py", line 287, in run_trial
    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/tuner.py", line 214, in _build_and_fit_model
    results = self.hypermodel.fit(hp, model, *args, **kwargs)
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/hypermodel.py", line 144, in fit
    return model.fit(*args, **kwargs)
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/tmp/__autograph_generated_file7go5p6qy.py", line 15, in tf__train_function
    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
tensorflow.python.autograph.pyct.error_utils.MultilineMessageKeyError: in user code:

    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/engine/training.py", line 1249, in train_function  *
        return step_function(self, iterator)
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/engine/training.py", line 1233, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/engine/training.py", line 1222, in run_step  **
        outputs = model.train_step(data)
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/engine/training.py", line 1027, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 527, in minimize
        self.apply_gradients(grads_and_vars)
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 1140, in apply_gradients
        return super().apply_gradients(grads_and_vars, name=name)
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 634, in apply_gradients
        iteration = self._internal_apply_gradients(grads_and_vars)
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 1166, in _internal_apply_gradients
        return tf.__internal__.distribute.interim.maybe_merge_call(
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 1216, in _distributed_apply_gradients_fn
        distribution.extended.update(
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 1213, in apply_grad_to_update_var  **
        return self._update_step(grad, var)
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 216, in _update_step
        raise KeyError(

    KeyError: 'The optimizer cannot recognize variable conv_0/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.{self.__class__.__name__}.'
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mRuntimeError[0m                              Traceback (most recent call last)
Cell [0;32mIn[32], line 4[0m
[1;32m      1[0m [38;5;28;01mfrom[39;00m [38;5;21;01mqkeras[39;00m[38;5;21;01m.[39;00m[38;5;21;01mautoqkeras[39;00m [38;5;28;01mimport[39;00m AutoQKeras
[1;32m      3[0m autoqk [38;5;241m=[39m AutoQKeras(baseline_model, output_dir[38;5;241m=[39m[38;5;124m"[39m[38;5;124mautoq_cnn[39m[38;5;124m"[39m, metrics[38;5;241m=[39m[[38;5;124m"[39m[38;5;124macc[39m[38;5;124m"[39m], custom_objects[38;5;241m=[39m{}, [38;5;241m*[39m[38;5;241m*[39mrun_config)
[0;32m----> 4[0m [43mautoqk[49m[38;5;241;43m.[39;49m[43mfit[49m[43m([49m[43mtrain_data[49m[43m,[49m[43m [49m[43mvalidation_data[49m[38;5;241;43m=[39;49m[43mval_data[49m[43m,[49m[43m [49m[43mepochs[49m[38;5;241;43m=[39;49m[38;5;241;43m15[39;49m[43m)[49m
[1;32m      6[0m aqmodel [38;5;241m=[39m autoqk[38;5;241m.[39mget_best_model()
[1;32m      7[0m print_qmodel_summary(aqmodel)

File [0;32m/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/qkeras/autoqkeras/autoqkeras_internal.py:968[0m, in [0;36mAutoQKeras.fit[0;34m(self, *fit_args, **fit_kwargs)[0m
[1;32m    962[0m   callbacks [38;5;241m=[39m callbacks [38;5;241m+[39m [
[1;32m    963[0m       tf[38;5;241m.[39mkeras[38;5;241m.[39mcallbacks[38;5;241m.[39mEarlyStopping(
[1;32m    964[0m           [38;5;124m"[39m[38;5;124mval_loss[39m[38;5;124m"[39m, patience[38;5;241m=[39m[38;5;28mmin[39m([38;5;241m20[39m, epochs[38;5;241m/[39m[38;5;241m/[39m[38;5;241m5[39m))
[1;32m    965[0m   ]
[1;32m    966[0m   fit_kwargs[[38;5;124m"[39m[38;5;124mcallbacks[39m[38;5;124m"[39m] [38;5;241m=[39m callbacks
[0;32m--> 968[0m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mtuner[49m[38;5;241;43m.[39;49m[43msearch[49m[43m([49m[38;5;241;43m*[39;49m[43mfit_args[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mfit_kwargs[49m[43m)[49m

File [0;32m/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py:231[0m, in [0;36mBaseTuner.search[0;34m(self, *fit_args, **fit_kwargs)[0m
[1;32m    229[0m     [38;5;28mself[39m[38;5;241m.[39mon_trial_begin(trial)
[1;32m    230[0m     [38;5;28mself[39m[38;5;241m.[39m_try_run_and_update_trial(trial, [38;5;241m*[39mfit_args, [38;5;241m*[39m[38;5;241m*[39mfit_kwargs)
[0;32m--> 231[0m     [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mon_trial_end[49m[43m([49m[43mtrial[49m[43m)[49m
[1;32m    232[0m [38;5;28mself[39m[38;5;241m.[39mon_search_end()

File [0;32m/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py:335[0m, in [0;36mBaseTuner.on_trial_end[0;34m(self, trial)[0m
[1;32m    329[0m [38;5;28;01mdef[39;00m [38;5;21mon_trial_end[39m([38;5;28mself[39m, trial):
[1;32m    330[0m [38;5;250m    [39m[38;5;124;03m"""Called at the end of a trial.[39;00m
[1;32m    331[0m 
[1;32m    332[0m [38;5;124;03m    Args:[39;00m
[1;32m    333[0m [38;5;124;03m        trial: A `Trial` instance.[39;00m
[1;32m    334[0m [38;5;124;03m    """[39;00m
[0;32m--> 335[0m     [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43moracle[49m[38;5;241;43m.[39;49m[43mend_trial[49m[43m([49m[43mtrial[49m[43m)[49m
[1;32m    336[0m     [38;5;66;03m# Display needs the updated trial scored by the Oracle.[39;00m
[1;32m    337[0m     [38;5;28mself[39m[38;5;241m.[39m_display[38;5;241m.[39mon_trial_end([38;5;28mself[39m[38;5;241m.[39moracle[38;5;241m.[39mget_trial(trial[38;5;241m.[39mtrial_id))

File [0;32m/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/oracle.py:107[0m, in [0;36msynchronized.<locals>.wrapped_func[0;34m(*args, **kwargs)[0m
[1;32m    105[0m     LOCKS[oracle][38;5;241m.[39macquire()
[1;32m    106[0m     THREADS[oracle] [38;5;241m=[39m thread_name
[0;32m--> 107[0m ret_val [38;5;241m=[39m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    108[0m [38;5;28;01mif[39;00m need_acquire:
[1;32m    109[0m     THREADS[oracle] [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/oracle.py:434[0m, in [0;36mOracle.end_trial[0;34m(self, trial)[0m
[1;32m    432[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m[38;5;241m.[39m_retry(trial):
[1;32m    433[0m     [38;5;28mself[39m[38;5;241m.[39mend_order[38;5;241m.[39mappend(trial[38;5;241m.[39mtrial_id)
[0;32m--> 434[0m     [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_check_consecutive_failures[49m[43m([49m[43m)[49m
[1;32m    436[0m [38;5;28mself[39m[38;5;241m.[39m_save_trial(trial)
[1;32m    437[0m [38;5;28mself[39m[38;5;241m.[39msave()

File [0;32m/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/oracle.py:386[0m, in [0;36mOracle._check_consecutive_failures[0;34m(self)[0m
[1;32m    384[0m     consecutive_failures [38;5;241m=[39m [38;5;241m0[39m
[1;32m    385[0m [38;5;28;01mif[39;00m consecutive_failures [38;5;241m==[39m [38;5;28mself[39m[38;5;241m.[39mmax_consecutive_failed_trials:
[0;32m--> 386[0m     [38;5;28;01mraise[39;00m [38;5;167;01mRuntimeError[39;00m(
[1;32m    387[0m         [38;5;124m"[39m[38;5;124mNumber of consecutive failures excceeded the limit [39m[38;5;124m"[39m
[1;32m    388[0m         [38;5;124mf[39m[38;5;124m"[39m[38;5;124mof [39m[38;5;132;01m{[39;00m[38;5;28mself[39m[38;5;241m.[39mmax_consecutive_failed_trials[38;5;132;01m}[39;00m[38;5;124m.[39m[38;5;130;01m\n[39;00m[38;5;124m"[39m
[1;32m    389[0m         [38;5;241m+[39m trial[38;5;241m.[39mmessage
[1;32m    390[0m     )

[0;31mRuntimeError[0m: Number of consecutive failures excceeded the limit of 3.
Traceback (most recent call last):
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py", line 270, in _try_run_and_update_trial
    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/base_tuner.py", line 235, in _run_and_update_trial
    results = self.run_trial(trial, *fit_args, **fit_kwargs)
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/tuner.py", line 287, in run_trial
    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/tuner.py", line 214, in _build_and_fit_model
    results = self.hypermodel.fit(hp, model, *args, **kwargs)
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras_tuner/engine/hypermodel.py", line 144, in fit
    return model.fit(*args, **kwargs)
  File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/tmp/__autograph_generated_file7go5p6qy.py", line 15, in tf__train_function
    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
tensorflow.python.autograph.pyct.error_utils.MultilineMessageKeyError: in user code:

    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/engine/training.py", line 1249, in train_function  *
        return step_function(self, iterator)
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/engine/training.py", line 1233, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/engine/training.py", line 1222, in run_step  **
        outputs = model.train_step(data)
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/engine/training.py", line 1027, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 527, in minimize
        self.apply_gradients(grads_and_vars)
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 1140, in apply_gradients
        return super().apply_gradients(grads_and_vars, name=name)
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 634, in apply_gradients
        iteration = self._internal_apply_gradients(grads_and_vars)
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 1166, in _internal_apply_gradients
        return tf.__internal__.distribute.interim.maybe_merge_call(
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 1216, in _distributed_apply_gradients_fn
        distribution.extended.update(
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 1213, in apply_grad_to_update_var  **
        return self._update_step(grad, var)
    File "/usr/share/miniconda/envs/hls4ml-tutorial/lib/python3.10/site-packages/keras/optimizers/optimizer_experimental/optimizer.py", line 216, in _update_step
        raise KeyError(

    KeyError: 'The optimizer cannot recognize variable conv_0/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.{self.__class__.__name__}.'



